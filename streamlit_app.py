import os
import streamlit as st
from PIL import Image
import pytesseract
import pandas as pd
from openai import OpenAI as oai
from pandasai import SmartDataframe
from pandasai.llm import OpenAI
from pandasai.responses.response_parser import ResponseParser

def load_file_content(file_path):
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            return file.read().strip()
    except Exception as e:
        st.error(f"L·ªói khi ƒë·ªçc file {file_path}: {e}")
        return ""

# Load dataset-related information
dataset_text = load_file_content("./data/dataset_info.txt")
extra_info = load_file_content("./data/extra_info.txt")
extra_info_2 = load_file_content("./data/extra_info_2.txt")
#density = load_file_content("./data/density.txt")
openai_insights = load_file_content("./data/openai_insights.txt")
#openai_insights_2 = load_file_content("./data/openai_insights_2.txt")
openai_summary = load_file_content("./data/openai_summary.txt")


class StreamlitResponse(ResponseParser):
    def __init__(self, context) -> None:
        super().__init__(context)

    def format_dataframe(self, result):
        st.dataframe(result["value"])
        return

    def format_plot(self, result):
        st.image(result["value"])
        return

    def format_other(self, result):
        st.write(result["value"])
        return

df = pd.read_csv('./data/data_dv.csv', encoding="utf-8")

st.write("Tr·ª£ l√Ω AI b√°o c√°o d·ªØ li·ªáu")

with st.expander("üîé Dataframe Preview"):
    st.write(df.tail(3))

from dotenv import load_dotenv
load_dotenv()

OPENAI_API_KEY= os.getenv("OPENAI_API_KEY")

llm = OpenAI(api_token=OPENAI_API_KEY)

query = st.text_area("üó£Ô∏è Chat with Dataframe")

client = oai(
    api_key=OPENAI_API_KEY,
)

def generate_openai_response(prompt, model="gpt-4o-mini", max_tokens=1500):
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh, c√≥ kh·∫£ nƒÉng tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn m·ªôt b·ªô d·ªØ li·ªáu l·ªõn."},
                {"role": "user", "content": prompt}
            ],
            #max_tokens=max_tokens,
            temperature=0.7,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"L·ªói khi g·ªçi API OpenAI: {e}"

def process_user_query(query, openai_answer):
    system_prompt = (
        f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh v·ªõi kh·∫£ nƒÉng tr·∫£ l·ªùi c√°c c√¢u h·ªèi ph·ª©c t·∫°p li√™n quan ƒë·∫øn b·ªô d·ªØ li·ªáu l·ªõn v√† ph√¢n t√≠ch s√¢u s·∫Øc. B·∫°n c√≥ kh·∫£ nƒÉng gi√∫p ng∆∞·ªùi d√πng hi·ªÉu r√µ h∆°n v·ªÅ c√°c th√¥ng tin v√† c√°c ph√¢n t√≠ch c√≥ s·∫µn, c≈©ng nh∆∞ h·ªó tr·ª£ h·ªç ƒë∆∞a ra c√°c quy·∫øt ƒë·ªãnh ch√≠nh x√°c v√† h·ª£p l√Ω d·ª±a tr√™n d·ªØ li·ªáu.

**Th√¥ng tin v·ªÅ b·ªô d·ªØ li·ªáu**:
{dataset_text}

**Nh·∫≠n x√©t t·ª´ con ng∆∞·ªùi**:
{extra_info}
{extra_info_2}

**Ph√¢n t√≠ch v√† nh·∫≠n x√©t t·ª´ OpenAI**:
{openai_insights}

**T√≥m t·∫Øt t·ªïng quan**:
{openai_summary}

**Nhi·ªám v·ª• c·ªßa b·∫°n**:
- X·ª≠ l√Ω v√† ch·ªçn l·ªçc c√°c th√¥ng tin quan tr·ªçng, ƒë·∫∑c bi·ªát ch√∫ tr·ªçng ƒë·∫øn nh·ªØng y·∫øu t·ªë c√≥ t√≠nh tr√πng l·∫∑p cao v√† b·∫£o ƒë·∫£m t√≠nh ch√≠nh x√°c trong c√°c ph√¢n t√≠ch.
- Cung c·∫•p c√°c d·ª± ƒëo√°n ho·∫∑c gi·∫£i th√≠ch d·ª±a tr√™n d·ªØ li·ªáu c√≥ s·∫µn v√† k·∫øt n·ªëi c√°c ph√¢n t√≠ch v·ªõi c√°c t√¨nh hu·ªëng th·ª±c t·∫ø. ƒê·∫£m b·∫£o r·∫±ng c√°c d·ª± ƒëo√°n n√†y d·ªÖ hi·ªÉu v√† c√≥ th·ªÉ gi√∫p ng∆∞·ªùi d√πng h√¨nh dung r√µ r√†ng v·ªÅ k·∫øt qu·∫£ c√≥ th·ªÉ x·∫£y ra.
- Ph√¢n t√≠ch v√† k·∫øt n·ªëi c√°c y·∫øu t·ªë d·ªØ li·ªáu ƒë·ªÉ t·∫°o ra nh·ªØng nh·∫≠n x√©t c√≥ chi·ªÅu s√¢u v√† h·ªØu √≠ch cho ng∆∞·ªùi d√πng. ƒê∆∞a ra c√°c nh·∫≠n x√©t b·ªï sung n·∫øu nh·∫≠n th·∫•y r·∫±ng ng∆∞·ªùi d√πng c√≥ th·ªÉ ch∆∞a nh·∫≠n ra m·ªôt m·ªëi li√™n h·ªá quan tr·ªçng n√†o ƒë√≥ trong d·ªØ li·ªáu.
- Khi tr·∫£ l·ªùi c√¢u h·ªèi, h√£y ƒë·∫£m b·∫£o r·∫±ng c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n r√µ r√†ng, d·ªÖ hi·ªÉu v√† m·∫°ch l·∫°c. ƒê·∫∑c bi·ªát ch√∫ tr·ªçng ƒë·∫øn vi·ªác gi·∫£i quy·∫øt c√°c c√¢u h·ªèi ph·ª©c t·∫°p ho·∫∑c m∆° h·ªì b·∫±ng c√°ch cung c·∫•p c√°c gi·∫£i th√≠ch chi ti·∫øt v√† d·ªÖ ti·∫øp c·∫≠n.
- N·∫øu c√¢u h·ªèi c√≥ s·ª± m∆° h·ªì ho·∫∑c kh√¥ng r√µ r√†ng, h√£y y√™u c·∫ßu ng∆∞·ªùi d√πng cung c·∫•p th√™m th√¥ng tin v√† l√†m r√µ c√°c y√™u c·∫ßu, ho·∫∑c ƒë·ªÅ xu·∫•t c√°c gi·∫£i ph√°p kh·∫£ thi m√† h·ªç c√≥ th·ªÉ tham kh·∫£o ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi ch√≠nh x√°c h∆°n.

**C√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng**: {query}

**C√¢u tr·∫£ l·ªùi m·∫´u t·ª´ OpenAI**: {openai_answer}

H√£y tr·∫£ l·ªùi m·ªôt c√°ch chi ti·∫øt, r√µ r√†ng v√† ch√≠nh x√°c b·∫±ng Ti·∫øng Vi·ªát. ƒê·∫£m b·∫£o r·∫±ng c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n kh√¥ng ch·ªâ ƒë·∫ßy ƒë·ªß m√† c√≤n c√≥ t√≠nh linh ho·∫°t, gi√∫p ng∆∞·ªùi d√πng kh√¥ng ch·ªâ tr·∫£ l·ªùi c√¢u h·ªèi m√† c√≤n kh√°m ph√° th√™m th√¥ng tin c√≥ gi√° tr·ªã t·ª´ b·ªô d·ªØ li·ªáu.
"""
    )
    return generate_openai_response(system_prompt)

if query:
    query_engine = SmartDataframe(
        df,
        config={
            "llm": llm,
            "response_parser": StreamlitResponse,
            #"custom_whitelisted_dependencies": None,
            # "callback": StreamlitCallback(container),
        },
    )
    user_query =  f"""
    **C√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng**: {query}

    H√£y tr·∫£ l·ªùi m·ªôt c√°ch chi ti·∫øt, r√µ r√†ng v√† ch√≠nh x√°c b·∫±ng Ti·∫øng Vi·ªát. ƒê·∫£m b·∫£o r·∫±ng c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n kh√¥ng ch·ªâ ƒë·∫ßy ƒë·ªß m√† c√≤n c√≥ t√≠nh linh ho·∫°t, gi√∫p ng∆∞·ªùi d√πng kh√¥ng ch·ªâ tr·∫£ l·ªùi c√¢u h·ªèi m√† c√≤n kh√°m ph√° th√™m th√¥ng tin c√≥ gi√° tr·ªã t·ª´ b·ªô d·ªØ li·ªáu.
    """

    openai_answer = query_engine.chat(user_query)
    answer = process_user_query(query,openai_answer)
    st.write(answer)